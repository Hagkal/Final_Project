# -*- coding: utf-8 -*-
"""Final Project - convolutional neural network for low level continuos features .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15J0MUiHK2UF74eMMJqrtfDPbpth9qlGj

## Imports
"""

# Commented out IPython magic to ensure Python compatibility.
from __future__ import print_function
import os
import math
import configparser

# Viz
import matplotlib
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)
import numpy as np

import ImageTransformer as imgt
import CNN

# settings
import warnings

warnings.filterwarnings("ignore")

matplotlib.use('Qt5Agg')


def get_dataset(data_file, label_file=None, csv=True, remote=True):
    data = None
    label = None
    file_extention = 'csv' if csv else 'dat'
    if remote:
        data = pd.read_csv(f'/content/gdrive/My Drive/Final Project/Code/DataSets/{data_file}.{file_extention}')
        if label_file is not None:
            label = pd.read_csv(f'/content/gdrive/My Drive/Final Project/Code/DataSets/{label_file}.{file_extention}')
    else:
        path = os.path.abspath(os.path.dirname(__file__))
        data = pd.read_csv(os.path.join(path, 'data', f'{data_file}.{file_extention}'))
        if label_file is not None:
            label = pd.read_csv(os.path.join(path, 'data', f'{label_file}.{file_extention}'))

    return data, label


"""Usage example"""

# bioresponse, redundant = get_dataset('Bioresponse')
# print(bioresponse.shape)

# cancer , red = get_dataset('gene expression cancer RNA-Seq/data', label_file='gene expression cancer RNA-Seq/labels')
# print(cancer.shape)
# print(red.shape)

"""## Data cleaning"""


# normalization
def drop_columns_with_one_value(df):
    df_copy = df.copy()
    for column in df.columns:
        if (len(df[column].unique()) == 1):
            df_copy.drop(column, inplace=True, axis=1)
    return df_copy


def normalize(df, method):
    if method == 'min-max':
        return (df - df.min()) / (df.max() - df.min())
    elif method == 'norm':
        return (df - df.mean()) / df.std()
    else:
        raise NotImplementedError('Other normalization not implemented yet')


"""Data splitting"""


def split_data(df_data, df_label, data_name):
    if data_name == 'Bioresponse':
        Bioresponse = drop_columns_with_one_value(df_data)
        Bioresponse = normalize(Bioresponse, 'min-max')
        Bioresponse_taret_0 = Bioresponse.loc[Bioresponse['target'] == 0]
        Bioresponse_taret_1 = Bioresponse.loc[Bioresponse['target'] == 1]
        Bioresponse_taret_0.shape, Bioresponse_taret_1.shape

        X_train_taret_0 = Bioresponse_taret_0.iloc[0:math.trunc(Bioresponse_taret_0.shape[0] * 0.6)]
        X_train_taret_1 = Bioresponse_taret_1.iloc[0:math.trunc(Bioresponse_taret_1.shape[0] * 0.6)]
        X_train = pd.concat([X_train_taret_0, X_train_taret_1], ignore_index=False, sort=False)
        X_train.shape

        X_validation_taret_0 = Bioresponse_taret_0.iloc[math.trunc(Bioresponse_taret_0.shape[0] * 0.6):math.trunc(
            Bioresponse_taret_0.shape[0] * 0.8)]
        X_validation_taret_1 = Bioresponse_taret_1.iloc[math.trunc(Bioresponse_taret_1.shape[0] * 0.6):math.trunc(
            Bioresponse_taret_1.shape[0] * 0.8)]
        X_validation = pd.concat([X_validation_taret_0, X_validation_taret_1], ignore_index=False, sort=False)
        X_validation.shape

        X_test_taret_0 = Bioresponse_taret_0.iloc[math.trunc(Bioresponse_taret_0.shape[0] * 0.8):]
        X_test_taret_1 = Bioresponse_taret_1.iloc[math.trunc(Bioresponse_taret_1.shape[0] * 0.8):]
        X_test = pd.concat([X_test_taret_0, X_test_taret_1], ignore_index=False, sort=False)

        y_train = X_train['target']
        y_validation = X_validation['target']
        y_test = X_test['target']

        X_train = X_train.drop('target', axis=1)
        X_validation = X_validation.drop('target', axis=1)
        X_test = X_test.drop('target', axis=1)

        X_train_transpose = X_train.T
        X_validation_transpose = X_validation.T
        X_test_transpose = X_test.T

        return {
            'train': {
                'original': X_train,
                'transpose': X_train.T,
                'labels': y_train
            },
            'validation': {
                'original': X_validation,
                'transpose': X_validation.T,
                'labels': y_validation
            },
            'test': {
                'original': X_test,
                'transpose': X_test.T,
                'labels': y_test
            }
        }

    elif data_name == 'gene_expression_cancer_data':
        gene_expression_cancer_data = df_data
        gene_expression_cancer_labels = df_label
        gene_expression_cancer_data.drop('Unnamed: 0', inplace=True, axis=1)
        gene_expression_cancer_data = drop_columns_with_one_value(gene_expression_cancer_data)
        gene_expression_cancer_data = normalize(gene_expression_cancer_data, 'min-max')
        gene_expression_cancer_data['target'] = gene_expression_cancer_labels['Class']
        classDictionary = {
            "PRAD": 0,
            "LUAD": 1,
            "BRCA": 2,
            "KIRC": 3,
            "COAD": 4
        }
        gene_expression_cancer_data = gene_expression_cancer_data.applymap(
            lambda s: classDictionary.get(s) if s in classDictionary else s)

        gene_expression_cancer_data_0 = gene_expression_cancer_data.loc[gene_expression_cancer_data['target'] == 0]
        gene_expression_cancer_data_1 = gene_expression_cancer_data.loc[gene_expression_cancer_data['target'] == 1]
        gene_expression_cancer_data_2 = gene_expression_cancer_data.loc[gene_expression_cancer_data['target'] == 2]
        gene_expression_cancer_data_3 = gene_expression_cancer_data.loc[gene_expression_cancer_data['target'] == 3]
        gene_expression_cancer_data_4 = gene_expression_cancer_data.loc[gene_expression_cancer_data['target'] == 4]

        X_train_target_0 = gene_expression_cancer_data_0.iloc[
                           0:math.trunc(gene_expression_cancer_data_0.shape[0] * 0.6)]
        X_train_target_1 = gene_expression_cancer_data_1.iloc[
                           0:math.trunc(gene_expression_cancer_data_1.shape[0] * 0.6)]
        X_train_target_2 = gene_expression_cancer_data_2.iloc[
                           0:math.trunc(gene_expression_cancer_data_2.shape[0] * 0.6)]
        X_train_target_3 = gene_expression_cancer_data_3.iloc[
                           0:math.trunc(gene_expression_cancer_data_3.shape[0] * 0.6)]
        X_train_target_4 = gene_expression_cancer_data_4.iloc[
                           0:math.trunc(gene_expression_cancer_data_4.shape[0] * 0.6)]
        X_train = pd.concat([X_train_target_0, X_train_target_1, X_train_target_2, X_train_target_3, X_train_target_4],
                            ignore_index=False, sort=False)

        X_validation_target_0 = gene_expression_cancer_data_0.iloc[
                                math.trunc(gene_expression_cancer_data_0.shape[0] * 0.6):math.trunc(
                                    gene_expression_cancer_data_0.shape[0] * 0.8)]
        X_validation_target_1 = gene_expression_cancer_data_1.iloc[
                                math.trunc(gene_expression_cancer_data_1.shape[0] * 0.6):math.trunc(
                                    gene_expression_cancer_data_1.shape[0] * 0.8)]
        X_validation_target_2 = gene_expression_cancer_data_2.iloc[
                                math.trunc(gene_expression_cancer_data_2.shape[0] * 0.6):math.trunc(
                                    gene_expression_cancer_data_2.shape[0] * 0.8)]
        X_validation_target_3 = gene_expression_cancer_data_3.iloc[
                                math.trunc(gene_expression_cancer_data_3.shape[0] * 0.6):math.trunc(
                                    gene_expression_cancer_data_3.shape[0] * 0.8)]
        X_validation_target_4 = gene_expression_cancer_data_4.iloc[
                                math.trunc(gene_expression_cancer_data_4.shape[0] * 0.6):math.trunc(
                                    gene_expression_cancer_data_4.shape[0] * 0.8)]
        X_validation = pd.concat(
            [X_validation_target_0, X_validation_target_1, X_validation_target_2, X_validation_target_3,
             X_validation_target_4], ignore_index=False, sort=False)

        X_test_target_0 = gene_expression_cancer_data_0.iloc[math.trunc(gene_expression_cancer_data_0.shape[0] * 0.8):]
        X_test_target_1 = gene_expression_cancer_data_1.iloc[math.trunc(gene_expression_cancer_data_1.shape[0] * 0.8):]
        X_test_target_2 = gene_expression_cancer_data_2.iloc[math.trunc(gene_expression_cancer_data_2.shape[0] * 0.8):]
        X_test_target_3 = gene_expression_cancer_data_3.iloc[math.trunc(gene_expression_cancer_data_3.shape[0] * 0.8):]
        X_test_target_4 = gene_expression_cancer_data_4.iloc[math.trunc(gene_expression_cancer_data_4.shape[0] * 0.8):]
        X_test = pd.concat([X_test_target_0, X_test_target_1, X_test_target_2, X_test_target_3, X_test_target_4],
                           ignore_index=False, sort=False)

        y_train = X_train['target']
        y_validation = X_validation['target']
        y_test = X_test['target']

        X_train = X_train.drop('target', axis=1).reset_index().drop('index', axis=1)
        X_validation = X_validation.drop('target', axis=1).reset_index().drop('index', axis=1)
        X_test = X_test.drop('target', axis=1).reset_index().drop('index', axis=1)

        X_train_transpose = X_train.T
        X_validation_transpose = X_validation.T
        X_test_transpose = X_test.T

        return {
            'train': {
                'original': X_train,
                'transpose': X_train.T,
                'labels': y_train
            },
            'validation': {
                'original': X_validation,
                'transpose': X_validation.T,
                'labels': y_validation
            },
            'test': {
                'original': X_test,
                'transpose': X_test.T,
                'labels': y_test
            }
        }


"""
# running tests
"""

# configuration reading
configs = configparser.ConfigParser()
configs.read('configurations.ini')

config_working_dataset = configs.get('dataset', 'name')
config_working_labels = configs.getboolean('dataset', 'label')
config_kpca_kernel = configs.get('kpca', 'used kernel')
config_resolution = configs.getint('image', 'resolution')
config_remote = configs.getboolean('dataset', 'remote')
config_num_of_classes = configs.getint('dataset', 'classes')

# reading the data
dataset, labels = get_dataset(config_working_dataset, remote=config_remote)
print(f'Dataset `{config_working_dataset}` shape is: {dataset.shape}')
if config_working_labels is not False:
    print(f'Dataset labels `{config_working_labels}` shape is: {labels.shape}')

# cleaning data and splitting to [train, validation, test]
splits = split_data(dataset, df_label=labels, data_name=config_working_dataset)

""""""
# unpacking train
train_x = splits['train']
validation_x = splits['validation']
test_x = splits['test']
print(f'Original dataset view: ')
print(train_x['original'].head(3))

print(f'\nTranspose dataset view: ')
print(train_x['transpose'].head(3))

# applying and plotting k-pca. for each feature in the training, assign X and Y as coordinates
kpca_point = imgt.KPCA(kernel=config_kpca_kernel, features_df=train_x['transpose'])
imgt.plot_kpca(kpca_point)

# constructing the image
features_to_pixels = imgt.divide_to_pixels(scatter=kpca_point, resolution=config_resolution)
pixels_to_features = imgt.to_pixel_map(features_to_pixels, resolution=config_resolution)
pixels_features_heat_map = imgt.feat_count_per_pixel(resolution=config_resolution, feat_in_pixels=features_to_pixels)
imgt.plot_heat_map(pixels_features_heat_map, fig_size=config_resolution)

# converting data for train
train_x['original'].reset_index(inplace=True)
validation_x['original'].reset_index(inplace=True)

X_train_as_image = imgt.df_to_array_of_images(train_x['original'], pixels_to_features, config_resolution,
                                              take_average_of_pixel=False)
X_validation_as_image = imgt.df_to_array_of_images(validation_x['original'], pixels_to_features, config_resolution,
                                                   take_average_of_pixel=False)

X_train_as_image = np.asarray(X_train_as_image)
X_validation_as_image = np.asarray(X_validation_as_image)

X_train_as_image = np.expand_dims(X_train_as_image, axis=3)
X_validation_as_image = np.expand_dims(X_validation_as_image, axis=3)

# create and train the model
model = CNN.create_model(config_resolution, config_resolution, config_num_of_classes)
history = CNN.train(model, X_train_as_image, train_x['labels'], X_validation_as_image, validation_x['labels'])

""""""


# cont'

def experiment_types_of_kernels(df):
    kernels = ['linear', 'poly', 'rbf', 'sigmoid', 'cosine']
    kernels = ['poly']

    for kernel in kernels:
        print(f'Experiment results for kernel: {kernel}')
        # applying and plotting k-pca
        kpca_point = imgt.KPCA(kernel=kernel, features_df=df)
        imgt.plot_kpca(kpca_point)
        imgt.save_scatter(kpca_point, imgt.create_info(config_working_dataset, 'kpca', f'(kernel={kernel})', 1))

        # constructing the image
        features_to_pixels = imgt.divide_to_pixels(scatter=kpca_point, resolution=config_resolution)
        pixels_to_features = imgt.to_pixel_map(features_to_pixels)
        pixels_features_heat_map = imgt.feat_count_per_pixel(resolution=config_resolution,
                                                             feat_in_pixels=features_to_pixels)
        imgt.plot_heat_map(pixels_features_heat_map, fig_size=config_resolution)

        print()


# experiment_types_of_kernels(train_x['transpose'])
